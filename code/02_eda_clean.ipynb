{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA & Pre-Processing\n",
    "\n",
    "### Notebook Summary\n",
    "In this notebook, I will be performing basic EDA on the text data that I pulled from Reddit in the previous notebook. I will also prepare the data for more extensive modeling in following notebooks by assembling the relevant text into a structured Pandas dataframe, scrubbing unwanted characters, and preparing vectorizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json, re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First I will load in the json files containing the raw Reddit data for all the posts I collected from the baseball and Dodgers subreddits in the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'../data/1540850144_raw_submissions.json', 'r') as f:\n",
    "    baseball_raw = json.load(f)\n",
    "    \n",
    "with open(f'../data/1499265759_raw_submissions.json', 'r') as f:\n",
    "    dodgers_raw = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA - Characters & Comments\n",
    "\n",
    "For some initial exploration I want to examine the character counts of the posts from both subreddits. I also want to examine the number of comments generated by each post to see how much engagement each post drives. The function in the following cell will receive a json file, iterate through every post stored in it, and append each post's character count and comment count to a respective list. When all the character and comment counts have been pulled, the lists will be combined into a dataframe and the function will return this dataframe. Posts without body text will trigger a KeyError in the function, so in the event that this occurs the function will increment a counter to keep track of how many posts have failed to compile. The function will print the value of this counter before returning the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chars_and_comments(posts):\n",
    "    char_count_list = []\n",
    "    comment_count_list = []\n",
    "    KeyError_counter = 0\n",
    "    temp_df = pd.DataFrame(columns=['char_count', 'num_comments'])\n",
    "    for i in range(len(posts)):\n",
    "        try:\n",
    "            char_count_list.append(len(posts[i]['selftext']))\n",
    "            comment_count_list.append(posts[i]['num_comments'])\n",
    "        except KeyError:\n",
    "            KeyError_counter += 1\n",
    "    print(f'There were {KeyError_counter} KeyErrors.')\n",
    "    temp_df['char_count'] = char_count_list\n",
    "    temp_df['num_comments'] = comment_count_list\n",
    "#     temp_df = pd.DataFrame(data=[char_count_list, comment_count_list], columns=['char_count', 'num_comments'])\n",
    "    return temp_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell I will pass the Dodgers and baseball jsons into the `chars_and_comments` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There were 892 KeyErrors.\n",
      "There were 3 KeyErrors.\n"
     ]
    }
   ],
   "source": [
    "dodgers_char_comm = chars_and_comments(dodgers_raw)\n",
    "baseball_char_comm = chars_and_comments(baseball_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only 3 of the 20,000 baseball posts triggered errors. I'm observing a fair number of KeyErrors from the Dodgers posts, but since the original quantity of posts was so large I still have more than 95% of the Dodgers posts to work with. Furthermore, the number of discarded posts is not so large as to result in unbalanced classes.\n",
    "\n",
    "Now I'll take a look at the summary statistics for the Dodgers character counts and comment counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>char_count</th>\n",
       "      <th>num_comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>19108.000000</td>\n",
       "      <td>19108.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>207.302177</td>\n",
       "      <td>50.424848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>795.807781</td>\n",
       "      <td>385.298749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>138.000000</td>\n",
       "      <td>17.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>14358.000000</td>\n",
       "      <td>18846.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         char_count  num_comments\n",
       "count  19108.000000  19108.000000\n",
       "mean     207.302177     50.424848\n",
       "std      795.807781    385.298749\n",
       "min        0.000000      0.000000\n",
       "25%        0.000000      1.000000\n",
       "50%        0.000000      6.000000\n",
       "75%      138.000000     17.000000\n",
       "max    14358.000000  18846.000000"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dodgers_char_comm.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's unfortunately a very wide range of values here, and it also seems that the numbers are being heavily weighted by a preponderance of posts with no body text. I'll take a closer look, this time at only those posts that have some text beyond the title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>char_count</th>\n",
       "      <th>num_comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>9121.000000</td>\n",
       "      <td>9121.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>434.286811</td>\n",
       "      <td>88.107664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1108.259412</td>\n",
       "      <td>553.676147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>28.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>153.000000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67%</th>\n",
       "      <td>278.000000</td>\n",
       "      <td>13.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>379.000000</td>\n",
       "      <td>18.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90%</th>\n",
       "      <td>894.000000</td>\n",
       "      <td>49.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>14358.000000</td>\n",
       "      <td>18846.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         char_count  num_comments\n",
       "count   9121.000000   9121.000000\n",
       "mean     434.286811     88.107664\n",
       "std     1108.259412    553.676147\n",
       "min        1.000000      0.000000\n",
       "25%       28.000000      1.000000\n",
       "50%      153.000000      6.000000\n",
       "67%      278.000000     13.000000\n",
       "75%      379.000000     18.000000\n",
       "90%      894.000000     49.000000\n",
       "max    14358.000000  18846.000000"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dodgers_char_comm[dodgers_char_comm['char_count'] > 0].describe(\n",
    "    percentiles=[.25, .5, .67, .75, .9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is encouraging from the perspective of social media outreach. Over half of the posts have no body text at all, and of those that do, two-thirds of them are within Twitter's 280-character limit. I'll take a look at the baseball posts' statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>char_count</th>\n",
       "      <th>num_comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>19997.000000</td>\n",
       "      <td>19997.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>261.506576</td>\n",
       "      <td>42.798020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1221.104661</td>\n",
       "      <td>92.357879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>89.000000</td>\n",
       "      <td>47.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>35914.000000</td>\n",
       "      <td>3747.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         char_count  num_comments\n",
       "count  19997.000000  19997.000000\n",
       "mean     261.506576     42.798020\n",
       "std     1221.104661     92.357879\n",
       "min        0.000000      0.000000\n",
       "25%        0.000000      3.000000\n",
       "50%        0.000000     15.000000\n",
       "75%       89.000000     47.000000\n",
       "max    35914.000000   3747.000000"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseball_char_comm.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the Dodgers posts, most of the baseball posts have no body text beyond their titles. I'll take a look at the numbers once I filter out the title-only posts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>char_count</th>\n",
       "      <th>num_comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>6602.000000</td>\n",
       "      <td>6602.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>792.085277</td>\n",
       "      <td>41.324599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2023.995852</td>\n",
       "      <td>78.420515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>93.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>246.000000</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67%</th>\n",
       "      <td>435.000000</td>\n",
       "      <td>36.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>592.000000</td>\n",
       "      <td>48.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90%</th>\n",
       "      <td>1773.000000</td>\n",
       "      <td>98.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>35914.000000</td>\n",
       "      <td>2230.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         char_count  num_comments\n",
       "count   6602.000000   6602.000000\n",
       "mean     792.085277     41.324599\n",
       "std     2023.995852     78.420515\n",
       "min        1.000000      0.000000\n",
       "25%       93.000000      5.000000\n",
       "50%      246.000000     19.000000\n",
       "67%      435.000000     36.000000\n",
       "75%      592.000000     48.000000\n",
       "90%     1773.000000     98.000000\n",
       "max    35914.000000   2230.000000"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseball_char_comm[baseball_char_comm['char_count'] > 0].describe(\n",
    "    percentiles=[.25, .5, .67, .75, .9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing these numbers to the Dodger posts' numbers are still encouraging from a social media perspective. Dodger fans seem to be less dependent on multimedia engagement since more of their posts contain body text, and while the baseball character counts quickly balloon beyond Twitter's character limit, more of the Dodger character counts are below 280 characters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `combine_text` function below will take a json of posts and iterate through it, extracting the values in the `title` and `selftext` features for each post and combining them into a single string for easier NLP analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_text(posts):\n",
    "    text_list = []\n",
    "    KeyError_counter = 0\n",
    "    for i in range(len(posts)):\n",
    "        try:\n",
    "            text_list.append(' '.join([posts[i]['title'], posts[i]['selftext']]))\n",
    "        except KeyError:\n",
    "            KeyError_counter += 1\n",
    "    print(f'There were {KeyError_counter} KeyErrors.')\n",
    "    return text_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell I will pass the Dodgers and baseball jsons into the `combine_text` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There were 892 KeyErrors.\n",
      "There were 3 KeyErrors.\n"
     ]
    }
   ],
   "source": [
    "dodgers_text = combine_text(dodgers_raw)\n",
    "baseball_text = combine_text(baseball_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that I've isolated the relevant text from each subreddit, I will pass each list of text into its own dataframe. Then I will add a target `dodgers` column to each dataframe to differentiate between the positive (Dodgers) and negative (baseball) classes. This target column in the `dodgers_df` dataframe will be filled with 1's and in the `baseball_df` dataframe it will be filled with all 0's.\n",
    "\n",
    "Once those two dataframes have been created, I will merge them together into a combined dataframe `df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "dodgers_df = pd.DataFrame(dodgers_text, columns=['text'])\n",
    "dodgers_df['dodgers'] = 1\n",
    "\n",
    "baseball_df = pd.DataFrame(baseball_text, columns=['text'])\n",
    "baseball_df['dodgers'] = 0\n",
    "\n",
    "df = pd.concat([dodgers_df, baseball_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start the text cleaning I will map a lambda function to the dataframe to change all of its text to a uniform lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df.text.map(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I don't want the model to be biased by the unbalanced use of the words \"Dodger\" or \"Dodgers\" in the Dodgers posts. To balance the influence of every team mention, I will replace every occurrence of any team name with a dummy word. In the following cell I will use a for loop to iterate through a set containing the names of all Major League Baseball teams and any common variations of those names. For each team name the loop will map a lambda function to the dataframe's `text` column and replace every instance of that team's name with the dummy word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlb_teams = {'diamondbacks', 'diamondback', 'dbacks', 'dback',\n",
    "             'braves', 'orioles', 'oriole', 'sox', 'cubs', 'reds',\n",
    "             'indians', 'indian', 'rockies', 'tigers', 'tiger',\n",
    "             'astros', 'astro', 'royals', 'royal', 'angels', 'angel',\n",
    "             'dodgers', 'dodger', 'marlins', 'marlin', 'brewers', 'brewer',\n",
    "             'twins', 'twin', 'yanks', 'yankees', 'yankee', 'mets',\n",
    "             'athletics', 'phillies', 'pirates', 'pirate',\n",
    "             'padres', 'padre', 'giants', 'giant', 'mariners', 'mariner',\n",
    "             'cardinals', 'cardinal', 'rays', 'ray', 'rangers', 'ranger',\n",
    "             'jays', 'nationals'}\n",
    "\n",
    "for team in mlb_teams:\n",
    "    df['text'] = df.text.map(lambda x: str.replace(x, team, 'team_ref'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next I will clean the dataframe's text by mapping a trio of lambda functions with regex strings. The functions will search for text patterns that match the regex strings and remove them from the dataframe.\n",
    "\n",
    "The first function will remove instances of \\[removed\\] and \\[deleted\\].\n",
    "\n",
    "The second function will remove any other instances where a post is fronted by bracketed text. For example, in posts that reference articles and breaking news, the name of the journalist who is reporting the story will often appear in brackets at the front of the post title. The second function will remove those instances.\n",
    "\n",
    "The third and final function will remove any remaining non-letter characters from the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df.text.map(lambda x: re.sub('\\[(removed|deleted)\\]', ' ', x))\n",
    "\n",
    "df['text'] = df.text.map(lambda x: re.sub('\\[([A-Za-z0-9_]+)\\]', ' ', x))\n",
    "\n",
    "df['text'] = df.text.map(lambda x: re.sub(\"[^a-zA-Z]\", \" \", x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The text is all prepped for vectorizing and modeling in the next notebooks. I will finish by storing the text in `X` and the classification targets in `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('dodgers', 1)\n",
    "y = df['dodgers']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the text and target data are ready, I will pickle them out so they can be easily loaded into other notebooks for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('../data/X_data.pkl', 'wb+') as f:\n",
    "#     pickle.dump(X, f)\n",
    "# with open('../data/y_data.pkl', 'wb+') as f:\n",
    "#     pickle.dump(y, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Vectorizers\n",
    "\n",
    "Before I can fit models to the text data, I need to convert the text into numeric data by using vectorizers. I will prepare the vectorizers now and pickle them out to be used in the modeling notebooks.\n",
    "\n",
    "### Stopwords\n",
    "\n",
    "When they are instantiated, each vectorizer will receive a list of stopwords. These stopwords will be ignored during vectorization so as to not bias the models with overly-influential words or confuse them with the noise of overly-common words.\n",
    "\n",
    "As a starting point, I will import the standard English stopwords from Natural Language Toolkit. These are common English words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I also want the vectorizers to ignore references to the cities of MLB teams and the abbreviations for each team. I have also included common words that are specific to this domain, like \"baseball\" and \"team.\" \"Pgt\" occurs frequently in the subreddits as an abbreviation for \"post-game thread,\" so I've included this in a list of additional stopwords.\n",
    "\n",
    "I will extend the default list of stopwords with the list of custom stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_stopwords = ['arizona', 'atlanta', 'baltimore', 'boston', 'chicago',\n",
    "                    'cincinnati', 'cinci', 'cleveland', 'colorado', 'detroit',\n",
    "                    'houston', 'kansas', 'los', 'angeles', 'la', 'miami',\n",
    "                    'milwaukee', 'minnesota', 'york', 'oakland', 'philadelphia',\n",
    "                    'philly', 'pittsburgh', 'san', 'diego', 'francisco', 'fran',\n",
    "                    'seattle', 'st.', 'louis', 'tampa', 'texas', 'toronto',\n",
    "                    'washington', 'ari', 'tal', 'bal', 'bos', 'chi', 'chc', 'cws',\n",
    "                    'cin', 'cle', 'col', 'det', 'hou', 'kc', 'lad', 'laa', 'mia',\n",
    "                    'mil', 'min', 'ny', 'nyy', 'nym', 'oak', 'phi', 'pit', 'sd',\n",
    "                    'sf', 'stl', 'tb', 'tex', 'tor', 'pgt', 'game', 'team',\n",
    "                    'player', 'players', 'mlb', 'baseball', 'tonight']\n",
    "\n",
    "stopwords.extend(custom_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the custom stopwords are ready, I can use them while instantiating the vectorizers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count Vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`CountVectorizer` is a simple way of converting text to numeric data. When fit to a collection of text, it will analyze the number of occurrences of individual words within that collection of text, and create a matrix of the specified `max_features` number of words along with their usage counts in each post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvec = CountVectorizer(max_features=500, stop_words=stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I'll pickle out the instantiated vectorizer for use in the next notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../assets/cvec.pkl', 'wb+') as f:\n",
    "    pickle.dump(cvec, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF\n",
    "\n",
    "Contrary to the `CountVectorizer`'s fairly straightforward approach, the `TfidfVectorizer` is a little more advanced. The \"term frequency\" (TF) of the vectorizer compares the ratio of the word's appearance frequency in a post to the overall number of words in that post. And the \"inverse document frequency\" (IDF) gives added predictive weight to rare words. In this instantiation of the vectorizer, a word will need to appear in at least 5 posts from the entire corpus, but not in more than 95% of them, to be considered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(stop_words=stopwords, min_df=5, max_df=.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I'll pickle out the instantiated vectorizer for use in a later modeling notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('../assets/tfidf_vec.pkl', 'wb+') as f:\n",
    "#     pickle.dump(tfidf, f)\n",
    "# with open('../assets/stopwords.pkl', 'wb+') as f:\n",
    "#     pickle.dump(stopwords, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm now ready to do some modeling on my data in the following notebooks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
