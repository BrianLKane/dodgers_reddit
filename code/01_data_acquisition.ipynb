{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Acquisition\n",
    "\n",
    "### Notebook Summary\n",
    "\n",
    "For this project I will be sourcing text data from Reddit. Due to the limits of the Reddit API, I will use the following code (lightly adapted from DSI-US-7 TA Chris Sinatra) to query the Pushshift API for thousands of posts from the r/Dodgers and r/baseball subreddits before storing the acquired data in json files to be used in the next notebooks for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, time, csv, json, re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `filename_format_log` function defined in the following cell will be called within the actual query function further below. When the query is finished and the function is closing out, it will call `filename_format_log` to write the name and directory path of the saved file, as well as the timestamp of the earliest post pulled as part of that query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filename_format_log(file_path, \n",
    "                        logfile = '../assets/file_log.txt', \n",
    "                        now = round(time.time()), \n",
    "                        file_description = None): \n",
    "   \n",
    "    try:\n",
    "        ext = re.search('(?<!^)(?<!\\.)\\.(?!\\.)', file_path).start() \n",
    "    except:\n",
    "        raise NameError('Please enter a relative path with a file extension.') \n",
    "    \n",
    "    stamp = re.search('(?<!^)(?<!\\.)[a-z]+_[a-z]+(?=\\.)', file_path).start()\n",
    "    formatted_name = f'{file_path[:stamp]}{now}_{file_path[stamp:]}'  \n",
    "    if not file_description:\n",
    "        file_description = f'Pull: {time.asctime(time.gmtime(now))}'\n",
    "    with open(logfile, 'a+') as f:\n",
    "        f.write(f'{formatted_name}: {file_description}\\n')\n",
    "    return formatted_name, now, file_description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function will take in a string argument to identify which subreddit should be queried. Unless otherwise specified by passing a specific value for `n_samples`, the function will pull 1500 total posts. The function can also take arguments for `before` and `after` to specify a window of time from which to pull the posts, otherwise it will default to pulling the most recent `n_samples` number of posts.\n",
    "\n",
    "Once it runs, it stores the current time in `last_post` and creates an empty list in which to store pulled posts. It then queries the designated subreddit. If the query doesn't turn up empty, it assigns the original submission time of the earliest pulled post to `last_post`, adds the newly-pulled posts to the storage list, pauses processing for one second (so as to not overload the host server) and continues interating through this process, pulling posts until there are no more uncollected posts in the targeted subreddit or until the number of posts designated in the function call has been collected.\n",
    "\n",
    "Finally, the function will call the `filename_format_log` function above to create a record of the successful query, and then it will store the collected data in a json file. As the function runs, it will print the loop's status at the start of each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reddit_query(subreddits, n_samples=1500, before=None, after=None):\n",
    "    url = f'https://api.pushshift.io/reddit/search/submission'\n",
    "    last_post = round(time.time())\n",
    "    post_list = []\n",
    "    \n",
    "    run = 1\n",
    "    while len(post_list) < n_samples:\n",
    "        \n",
    "        try:\n",
    "            print(f'Starting query {run}')\n",
    "            \n",
    "            params = {\n",
    "              'subreddit':subreddits,\n",
    "              'sort':'desc',\n",
    "              'size':n_samples,\n",
    "              'before':last_post-1,\n",
    "              'after':after,\n",
    "             }\n",
    "                \n",
    "            response = requests.get(url, params = params)\n",
    "            posts = response.json()['data']\n",
    "            \n",
    "            if len(posts) == 0:\n",
    "                last_post = last_post\n",
    "            else:\n",
    "                last_post = posts[-1]['created_utc']\n",
    "                post_list.extend(posts)\n",
    "                timestamp = posts[-1]['created_utc']\n",
    "                time.sleep(1) \n",
    "                run += 1\n",
    "        except:\n",
    "            if response.status_code != 200:\n",
    "                return f'Check status. Error code: {response.status_code}'\n",
    "            else:\n",
    "                return 'Error. Pull not completed.'\n",
    "    \n",
    "    formatted_name, now, file_description = filename_format_log(file_path =f'../data/raw_submissions.json', now=timestamp)\n",
    "    with open(formatted_name, 'w+') as f:\n",
    "        json.dump(post_list, f)\n",
    "    \n",
    "    print(f'Saved and completed query and returned {len(post_list)} submissions.')\n",
    "    print(f'Reddit text is ready for processing.')\n",
    "    return print(f'Last timestamp was {timestamp}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I will use the `reddit_query` function to collect 20,000 posts from the r/baseball subreddit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_query(subreddits='baseball', n_samples=20000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I will use the `reddit_query` function to collect 20,000 posts from the /Dodgers subreddit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting query 1\n",
      "Starting query 2\n",
      "Starting query 3\n",
      "Starting query 4\n",
      "Starting query 5\n",
      "Starting query 6\n",
      "Starting query 7\n",
      "Starting query 8\n",
      "Starting query 9\n",
      "Starting query 10\n",
      "Starting query 11\n",
      "Starting query 12\n",
      "Starting query 13\n",
      "Starting query 14\n",
      "Starting query 15\n",
      "Starting query 16\n",
      "Starting query 17\n",
      "Starting query 18\n",
      "Starting query 19\n",
      "Starting query 20\n",
      "Saved and completed query and returned 20000 submissions.\n",
      "Reddit text is ready for processing.\n",
      "Last timestamp was 1499265759.\n"
     ]
    }
   ],
   "source": [
    "reddit_query(subreddits='Dodgers', n_samples=20000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that I have some data, I will move on to EDA and pre-processing in the next notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
